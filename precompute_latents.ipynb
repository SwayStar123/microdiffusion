{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from diffusers import AutoencoderKL\n",
    "import torch\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.config.HF_HUB_OFFLINE = 1 # Comment this out if you havent downloaded the dataset yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since tpremoli/CelebA-attrs couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'default' at ..\\..\\datasets\\CelebA-attrs\\tpremoli___celeb_a-attrs\\default\\0.0.0\\ed9021d2871ceddbd3cf0fb642544bd7c60c5152 (last modified on Fri Oct  4 14:25:53 2024).\n",
      "Using the latest cached version of the dataset since tpremoli/CelebA-attrs couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'default' at ..\\..\\datasets\\CelebA-attrs\\tpremoli___celeb_a-attrs\\default\\0.0.0\\ed9021d2871ceddbd3cf0fb642544bd7c60c5152 (last modified on Fri Oct  4 14:25:53 2024).\n",
      "Using the latest cached version of the dataset since tpremoli/CelebA-attrs couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'default' at ..\\..\\datasets\\CelebA-attrs\\tpremoli___celeb_a-attrs\\default\\0.0.0\\ed9021d2871ceddbd3cf0fb642544bd7c60c5152 (last modified on Fri Oct  4 14:25:53 2024).\n"
     ]
    }
   ],
   "source": [
    "train_ds = load_dataset(\"tpremoli/CelebA-attrs\", cache_dir=\"../../datasets/CelebA-attrs\", split=\"train\")\n",
    "validation_ds = load_dataset(\"tpremoli/CelebA-attrs\", cache_dir=\"../../datasets/CelebA-attrs\", split=\"validation\")\n",
    "test_ds = load_dataset(\"tpremoli/CelebA-attrs\", cache_dir=\"../../datasets/CelebA-attrs\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 218)\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0][\"image\"].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 176)),  # Resize to 176x224 (Height x Width)\n",
    "    transforms.ToTensor(),           # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Scale to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", cache_dir=\"../../models/vae\")\n",
    "vae = vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform2 = lambda x: vae.encode(x.to(device).unsqueeze(0)).latent_dist.sample().squeeze(0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train:   0%|          | 0/5087 [00:00<?, ?it/s]d:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\diffusers\\models\\attention_processor.py:2358: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  hidden_states = F.scaled_dot_product_attention(\n",
      "Processing train: 100%|██████████| 5087/5087 [25:31<00:00,  3.32it/s]\n",
      "Processing validation: 100%|██████████| 624/624 [03:04<00:00,  3.37it/s]\n",
      "Processing test: 100%|██████████| 621/621 [03:03<00:00,  3.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAVE_PATH = \"../../datasets/CelebA-attrs-latents\"\n",
    "\n",
    "# def process_ds(ds, set):\n",
    "#     dict = {}\n",
    "#     for i in range(len(ds)):\n",
    "#         sample = ds[i]\n",
    "#         latents = transform2(transform(sample[\"image\"])).to(torch.float16)\n",
    "#         dict[i] = sample[\"prompt_string\"]\n",
    "#         # Pad filename to 8 digits\n",
    "#         filename = f\"{i:08d}.pt\"\n",
    "#         torch.save(latents, f\"{SAVE_PATH}/{set}/latents/{filename}\")\n",
    "\n",
    "#     with open(f\"{SAVE_PATH}/{set}/metadata.json\", \"w\") as f:\n",
    "#         json.dump(dict, f)    \n",
    "\n",
    "def process_ds(ds, set_name, batch_size=32):\n",
    "    \"\"\"\n",
    "    Processes the dataset in batches, encodes images using the VAE,\n",
    "    saves latent vectors, and records metadata.\n",
    "\n",
    "    Args:\n",
    "        ds (Dataset): The dataset to process.\n",
    "        set_name (str): The name of the dataset split (e.g., 'train', 'validation', 'test').\n",
    "        batch_size (int, optional): Number of samples to process in each batch. Defaults to 32.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    num_samples = len(ds)\n",
    "    SAVE_LATENTS_DIR = f\"{SAVE_PATH}/{set_name}/latents\"\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(SAVE_LATENTS_DIR, exist_ok=True)\n",
    "\n",
    "    # Process the dataset in batches\n",
    "    for start_idx in tqdm(range(0, num_samples, batch_size), desc=f\"Processing {set_name}\"):\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        batch_indices = range(start_idx, end_idx)\n",
    "        \n",
    "        # Load and transform images\n",
    "        images = [transform(ds[i][\"image\"]) for i in batch_indices]\n",
    "        prompts = [ds[i][\"prompt_string\"] for i in batch_indices]\n",
    "        \n",
    "        # Stack images into a batch tensor\n",
    "        batch_tensor = torch.stack(images).to(device)  # Shape: (batch_size, 3, 216, 176)\n",
    "        \n",
    "        # Encode the batch using the VAE\n",
    "        with torch.no_grad():\n",
    "            # Encode the batch and sample latent vectors\n",
    "            encoded = vae.encode(batch_tensor)\n",
    "            latents = encoded.latent_dist.sample().cpu().half()  # Shape: (batch_size, latent_dim)\n",
    "        \n",
    "        # Save each latent vector and update metadata\n",
    "        for i, latent in enumerate(latents):\n",
    "            idx = start_idx + i\n",
    "            metadata[idx] = prompts[i]\n",
    "            filename = f\"{idx:08d}.pt\"\n",
    "            torch.save(latent.clone().detach(), f\"{SAVE_LATENTS_DIR}/{filename}\")\n",
    "    \n",
    "    # Save metadata to JSON\n",
    "    metadata_path = f\"{SAVE_PATH}/{set_name}/metadata.json\"\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "process_ds(train_ds, \"train\")\n",
    "process_ds(validation_ds, \"validation\")\n",
    "process_ds(test_ds, \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
